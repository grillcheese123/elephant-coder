{"ts": "2026-02-09T00:18:50Z", "session_id": "full_strategy_20260209a_bugfix_02_elephant_r1", "role": "user", "command": "/code", "run_id": "run_5909a024d053", "content": "Fix stats aggregation to handle malformed run records without crashing."}
{"ts": "2026-02-09T00:18:50Z", "session_id": "full_strategy_20260209a_bugfix_02_elephant_r1", "role": "assistant", "command": "/code", "run_id": "run_5909a024d053", "content": "{\"summary\": \"The stats aggregation logic in benchmark_runner.py crashed due to malformed run records (e.g., missing fields like 'cmd' or 'ok'). The fix adds defensive parsing to skip invalid records and ensures aggregation functions handle missing or invalid data gracefully.\", \"project\": {\"language\": \"python\"}, \"files\": [{\"path\": \"scripts/benchmark_runner.py\", \"content\": \"#!/usr/bin/env python3\\n\\\"\\\"\\\"Benchmark runner for Elephant Coder protocol v1.\\\"\\\"\\\"\\n\\nfrom __future__ import annotations\\n\\nimport argparse\\nimport importlib.util\\nimport json\\nimport os\\nimport subprocess\\nimport sys\\nfrom dataclasses import dataclass\\nfrom datetime import datetime, timezone\\nfrom pathlib import Path\\nfrom statistics import mean\\nfrom typing import Any\\n\\n\\ndef _utc_now_iso() -> str:\\n    return datetime.now(timezone.utc).strftime(\\\"%Y-%m-%dT%H:%M:%SZ\\\")\\n\\n\\ndef _default_benchmark_id() -> str:\\n    stamp = datetime.now(timezone.utc).strftime(\\\"%Y%m%dT%H%M%SZ\\\")\\n    return f\\\"bench_{stamp}_{os.getpid()}\\\"\\n\\n\\ndef _sanitize_id(raw: str) -> str:\\n    value = \\\"\\\".join(ch for ch in str(raw).strip() if ch.isalnum() or ch in {\\\"-\\\", \\\"_\\\"})\\n    if not value:\\n        raise ValueError(\\\"benchmark/session id must contain at least one valid character\\\")\\n    return value\\n\\n\\ndef _session_log_path(project_root: Path, session_id: str) -> Path:\\n    safe = _sanitize_id(session_id)\\n    return project_root / \\\".elephant-coder\\\" / \\\"sessions\\\" / f\\\"{safe}.jsonl\\\"\\n\\n\\ndef _load_cli_module(project_root: Path):\\n    cli_path = project_root / \\\"scripts\\\" / \\\"elephant_cli.py\\\"\\n    spec = importlib.util.spec_from_file_location(\\\"elephant_cli\\\", cli_path)\\n    if spec and spec.loader:\\n        module = importlib.util.module_from_spec(spec)\\n        spec.loader.exec_module(module)\\n        return module\\n    raise RuntimeError(\\\"Could not load elephant_cli.py\\\")\\n\\n\\ndef _parse_run_record(line: str) -> dict[str, Any] | None:\\n    \\\"\\\"\\\"Parse a single JSONL run record, returning None if malformed.\\\"\\\"\\\"\\n    try:\\n        record = json.loads(line)\\n        if not isinstance(record, dict):\\n            return None\\n        # Require at least 'run' and 'cmd' fields\\n        if \\\"run\\\" not in record or \\\"cmd\\\" not in record:\\n            return None\\n        return record\\n    except (json.JSONDecodeError, TypeError):\\n        return None\\n\\n\\ndef _safe_float(value: Any, default: float = 0.0) -> float:\\n    \\\"\\\"\\\"Safely convert a value to float, returning default if invalid.\\\"\\\"\\\"\\n    try:\\n        return float(value)\\n    except (TypeError, ValueError):\\n        return default\\n\\n\\ndef _safe_bool(value: Any, default: bool = False) -> bool:\\n    \\\"\\\"\\\"Safely convert a value to bool, returning default if invalid.\\\"\\\"\\\"\\n    if isinstance(value, bool):\\n        return value\\n    if isinstance(value, str):\\n        return value.lower() in {\\\"true\\\", \\\"1\\\", \\\"yes\\\"}\\n    return default\\n\\n\\ndef aggregate_stats_from_session(session_path: Path) -> dict[str, Any]:\\n    \\\"\\\"\\\"Aggregate benchmark stats from a session log file, skipping malformed records.\\\"\\\"\\\"\\n    runs: list[dict[str, Any]] = []\\n    if not session_path.exists():\\n        return {\\\"count\\\": 0, \\\"success_count\\\": 0, \\\"fail_count\\\": 0, \\\"avg_latency_ms\\\": 0.0, \\\"avg_cost_usd\\\": 0.0}\\n\\n    with session_path.open(\\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n        for line in f:\\n            line = line.strip()\\n            if not line:\\n                continue\\n            record = _parse_run_record(line)\\n            if record is None:\\n                continue\\n            runs.append(record)\\n\\n    success_count = sum(1 for r in runs if _safe_bool(r.get(\\\"ok\\\"), False))\\n    fail_count = len(runs) - success_count\\n    latencies = [_safe_float(r.get(\\\"latency_ms\\\")) for r in runs]\\n    costs = [_safe_float(r.get(\\\"cost_usd\\\")) for r in runs]\\n\\n    return {\\n        \\\"count\\\": len(runs),\\n        \\\"success_count\\\": success_count,\\n        \\\"fail_count\\\": fail_count,\\n        \\\"avg_latency_ms\\\": mean(latencies) if latencies else 0.0,\\n        \\\"avg_cost_usd\\\": mean(costs) if costs else 0.0,\\n    }\\n\\n\\ndef run_benchmark(project_root: Path, session_id: str, max_runs: int = 10) -> None:\\n    \\\"\\\"\\\"Run the benchmark suite and log results.\\\"\\\"\\\"\\n    cli = _load_cli_module(project_root)\\n    session_log = _session_log_path(project_root, session_id)\\n    session_log.parent.mkdir(parents=True, exist_ok=True)\\n\\n    for i in range(max_runs):\\n        try:\\n            result = cli.main([\\\"--code\\\", \\\"--quiet\\\"])\\n            ok = result == 0\\n        except Exception as e:\\n            ok = False\\n            result = str(e)\\n\\n        record = {\\n            \\\"run\\\": f\\\"run_{i:04d}\\\",\\n            \\\"cmd\\\": \\\"--code --quiet\\\",\\n            \\\"ok\\\": ok,\\n            \\\"latency_ms\\\": 0.0,\\n            \\\"tokens\\\": 0,\\n            \\\"cost_usd\\\": 0.0,\\n            \\\"timestamp\\\": _utc_now_iso(),\\n        }\\n        with session_log.open(\\\"a\\\", encoding=\\\"utf-8\\\") as f:\\n            f.write(json.dumps(record) + \\\"\\\\n\\\")\\n\\n    stats = aggregate_stats_from_session(session_log)\\n    print(f\\\"Benchmark complete. Stats: {stats}\\\")\\n\\n\\ndef main() -> int:\\n    parser = argparse.ArgumentParser(description=\\\"Elephant Coder benchmark runner\\\")\\n    parser.add_argument(\\\"--session\\\", default=_default_benchmark_id(), help=\\\"Session ID\\\")\\n    parser.add_argument(\\\"--max-runs\\\", type=int, default=10, help=\\\"Max runs\\\")\\n    args = parser.parse_args()\\n\\n    project_root = Path.cwd()\\n    run_benchmark(project_root, args.session, args.max_runs)\\n    return 0\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    sys.exit(main())\\n\"}], \"dependencies\": {\"dependencies\": {}, \"devDependencies\": {}, \"python\": [], \"pythonDev\": [], \"rust\": [], \"go\": [], \"cpp\": []}, \"commands\": [\"python scripts/benchmark_runner.py --session test_fix --max-runs 5\"]}"}
