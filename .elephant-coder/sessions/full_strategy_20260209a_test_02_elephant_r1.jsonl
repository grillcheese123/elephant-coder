{"ts": "2026-02-09T00:25:46Z", "session_id": "full_strategy_20260209a_test_02_elephant_r1", "role": "user", "command": "/code", "run_id": "run_bebf40102bc5", "content": "Write tests for model retry and fallback telemetry in /code responses."}
{"ts": "2026-02-09T00:25:46Z", "session_id": "full_strategy_20260209a_test_02_elephant_r1", "role": "assistant", "command": "/code", "run_id": "run_bebf40102bc5", "content": "{\"summary\": \"Write tests for model retry and fallback telemetry in /code responses.\", \"project\": {\"language\": \"python\"}, \"files\": [{\"path\": \"tests/test_model_retry_fallback.py\", \"content\": \"\\\"\\\"\\\"Tests for model retry and fallback telemetry in /code responses.\\\"\\\"\\\"\\n\\nfrom __future__ import annotations\\n\\nimport json\\nfrom pathlib import Path\\nfrom unittest.mock import MagicMock, patch\\n\\nimport pytest\\n\\n\\ndef _load_runner_module():\\n    root = Path(__file__).resolve().parents[1]\\n    runner_path = root / \\\"scripts\\\" / \\\"benchmark_runner.py\\\"\\n    import importlib.util\\n    import sys\\n    spec = importlib.util.spec_from_file_location(\\\"benchmark_runner_local\\\", runner_path)\\n    if spec is None or spec.loader is None:\\n        raise RuntimeError(\\\"Unable to load scripts/benchmark_runner.py\\\")\\n    module = importlib.util.module_from_spec(spec)\\n    sys.modules[spec.name] = module\\n    spec.loader.exec_module(module)\\n    return module\\n\\n\\nbenchmark_runner = _load_runner_module()\\n\\n\\nclass MockRunResult:\\n    \\\"\\\"\\\"Mock for run.RunResult to simulate telemetry data.\\\"\\\"\\\"\\n    def __init__(self, run_id: str, ok: bool, latency_ms: int, tokens: int, cost_usd: float, metadata: dict | None = None):\\n        self.run_id = run_id\\n        self.ok = ok\\n        self.latency_ms = latency_ms\\n        self.tokens = tokens\\n        self.cost_usd = cost_usd\\n        self.metadata = metadata or {}\\n\\n\\nclass MockRun:\\n    \\\"\\\"\\\"Mock for run.Run to simulate telemetry data.\\\"\\\"\\\"\\n    def __init__(self, result: MockRunResult):\\n        self.result = result\\n\\n\\nclass MockSession:\\n    \\\"\\\"\\\"Mock for telemetry.Session to simulate telemetry session.\\\"\\\"\\\"\\n    def __init__(self):\\n        self.events: list[dict] = []\\n\\n    def record(self, event: dict):\\n        self.events.append(event)\\n\\n\\ndef test_retry_event_emitted_on_failure():\\n    \\\"\\\"\\\"Test that a retry event is emitted when a /code request fails and is retried.\\\"\\\"\\\"\\n    mock_result = MockRunResult(\\\"run_retry_001\\\", ok=False, latency_ms=1000, tokens=100, cost_usd=0.001)\\n    mock_run = MockRun(mock_result)\\n    mock_session = MockSession()\\n\\n    with patch(\\\"elephant_coder.openrouter_client.OpenRouterClient.chat_completion\\\") as mock_chat:\\n        # Simulate two failures then success\\n        mock_chat.side_effect = [\\n            RuntimeError(\\\"API error\\\"),\\n            RuntimeError(\\\"API error\\\"),\\n            {\\\"choices\\\": [{\\\"message\\\": {\\\"content\\\": \\\"Success\\\"}}], \\\"usage\\\": {\\\"total_tokens\\\": 50}},\\n        ]\\n\\n        # Simulate retry logic with telemetry\\n        max_retries = 3\\n        attempt = 0\\n        while attempt < max_retries:\\n            try:\\n                result = mock_chat()\\n                break\\n            except RuntimeError:\\n                attempt += 1\\n                mock_session.record({\\n                    \\\"event\\\": \\\"model_retry\\\",\\n                    \\\"run_id\\\": mock_result.run_id,\\n                    \\\"attempt\\\": attempt,\\n                    \\\"error\\\": \\\"API error\\\",\\n                })\\n        else:\\n            raise RuntimeError(\\\"Max retries exceeded\\\")\\n\\n    assert len(mock_session.events) == 2\\n    assert mock_session.events[0][\\\"event\\\"] == \\\"model_retry\\\"\\n    assert mock_session.events[0][\\\"attempt\\\"] == 1\\n    assert mock_session.events[1][\\\"attempt\\\"] == 2\\n\\n\\ndef test_fallback_event_emitted_on_fallback_model():\\n    \\\"\\\"\\\"Test that a fallback event is emitted when a fallback model is used.\\\"\\\"\\\"\\n    mock_result = MockRunResult(\\\"run_fallback_001\\\", ok=True, latency_ms=2000, tokens=200, cost_usd=0.002)\\n    mock_session = MockSession()\\n\\n    # Simulate fallback logic\\n    primary_model = \\\"gpt-4\\\"\\n    fallback_model = \\\"gpt-3.5-turbo\\\"\\n    used_model = fallback_model\\n\\n    mock_session.record({\\n        \\\"event\\\": \\\"model_fallback\\\",\\n        \\\"run_id\\\": mock_result.run_id,\\n        \\\"primary_model\\\": primary_model,\\n        \\\"fallback_model\\\": fallback_model,\\n        \\\"used_model\\\": used_model,\\n    })\\n\\n    assert len(mock_session.events) == 1\\n    assert mock_session.events[0][\\\"event\\\"] == \\\"model_fallback\\\"\\n    assert mock_session.events[0][\\\"primary_model\\\"] == \\\"gpt-4\\\"\\n    assert mock_session.events[0][\\\"fallback_model\\\"] == \\\"gpt-3.5-turbo\\\"\\n\\n\\ndef test_retry_and_fallback_combined():\\n    \\\"\\\"\\\"Test that both retry and fallback events are recorded when applicable.\\\"\\\"\\\"\\n    mock_result = MockRunResult(\\\"run_combined_001\\\", ok=False, latency_ms=500, tokens=50, cost_usd=0.0005)\\n    mock_session = MockSession()\\n\\n    # Simulate retry with fallback\\n    max_retries = 2\\n    attempt = 0\\n    fallback_model = \\\"gpt-3.5-turbo\\\"\\n\\n    while attempt < max_retries:\\n        try:\\n            raise RuntimeError(\\\"API error\\\")\\n        except RuntimeError:\\n            attempt += 1\\n            mock_session.record({\\n                \\\"event\\\": \\\"model_retry\\\",\\n                \\\"run_id\\\": mock_result.run_id,\\n                \\\"attempt\\\": attempt,\\n                \\\"error\\\": \\\"API error\\\",\\n                \\\"used_model\\\": fallback_model if attempt > 1 else None,\\n            })\\n\\n    # Record fallback event\\n    mock_session.record({\\n        \\\"event\\\": \\\"model_fallback\\\",\\n        \\\"run_id\\\": mock_result.run_id,\\n        \\\"primary_model\\\": \\\"gpt-4\\\",\\n        \\\"fallback_model\\\": fallback_model,\\n        \\\"used_model\\\": fallback_model,\\n    })\\n\\n    assert len(mock_session.events) == 3\\n    retry_events = [e for e in mock_session.events if e[\\\"event\\\"] == \\\"model_retry\\\"]\\n    fallback_events = [e for e in mock_session.events if e[\\\"event\\\"] == \\\"model_fallback\\\"]\\n    assert len(retry_events) == 2\\n    assert len(fallback_events) == 1\\n\\n\\ndef test_telemetry_payload_structure():\\n    \\\"\\\"\\\"Test that telemetry payloads have the expected structure.\\\"\\\"\\\"\\n    mock_result = MockRunResult(\\\"run_structure_001\\\", ok=True, latency_ms=1500, tokens=150, cost_usd=0.0015)\\n    mock_session = MockSession()\\n\\n    # Record events with expected structure\\n    mock_session.record({\\n        \\\"event\\\": \\\"model_retry\\\",\\n        \\\"run_id\\\": mock_result.run_id,\\n        \\\"attempt\\\": 1,\\n        \\\"error\\\": \\\"API error\\\",\\n        \\\"timestamp\\\": benchmark_runner._utc_now_iso(),\\n    })\\n\\n    mock_session.record({\\n        \\\"event\\\": \\\"model_fallback\\\",\\n        \\\"run_id\\\": mock_result.run_id,\\n        \\\"primary_model\\\": \\\"gpt-4\\\",\\n        \\\"fallback_model\\\": \\\"gpt-3.5-turbo\\\",\\n        \\\"timestamp\\\": benchmark_runner._utc_now_iso(),\\n    })\\n\\n    for event in mock_session.events:\\n        assert \\\"event\\\" in event\\n        assert \\\"run_id\\\" in event\\n        assert \\\"timestamp\\\" in event\\n\\n    retry_event = next(e for e in mock_session.events if e[\\\"event\\\"] == \\\"model_retry\\\")\\n    assert \\\"attempt\\\" in retry_event\\n    assert \\\"error\\\" in retry_event\\n\\n    fallback_event = next(e for e in mock_session.events if e[\\\"event\\\"] == \\\"model_fallback\\\")\\n    assert \\\"primary_model\\\" in fallback_event\\n    assert \\\"fallback_model\\\" in fallback_event\\n\\n\\ndef test_telemetry_integration_with_benchmark_runner():\\n    \\\"\\\"\\\"Test that telemetry is properly integrated with benchmark runner.\\\"\\\"\\\"\\n    mock_result = MockRunResult(\\\"run_integration_001\\\", ok=True, latency_ms=1000, tokens=100, cost_usd=0.001)\\n    mock_session = MockSession()\\n\\n    # Simulate benchmark runner telemetry integration\\n    telemetry_data = {\\n        \\\"run_id\\\": mock_result.run_id,\\n        \\\"ok\\\": mock_result.ok,\\n        \\\"latency_ms\\\": mock_result.latency_ms,\\n        \\\"tokens\\\": mock_result.tokens,\\n        \\\"cost_usd\\\": mock_result.cost_usd,\\n        \\\"metadata\\\": {\\n            \\\"retries\\\": 2,\\n            \\\"fallback_used\\\": True,\\n        },\\n    }\\n\\n    mock_session.record({\\n        \\\"event\\\": \\\"benchmark_run\\\",\\n        **telemetry_data,\\n    })\\n\\n    assert len(mock_session.events) == 1\\n    event = mock_session.events[0]\\n    assert event[\\\"event\\\"] == \\\"benchmark_run\\\"\\n    assert event[\\\"metadata\\\"][\\\"retries\\\"] == 2\\n    assert event[\\\"metadata\\\"][\\\"fallback_used\\\"] is True\\n\\n\\ndef test_telemetry_persistence_to_jsonl():\\n    \\\"\\\"\\\"Test that telemetry events can be persisted to JSONL format.\\\"\\\"\\\"\\n    mock_result = MockRunResult(\\\"run_persist_001\\\", ok=True, latency_ms=1200, tokens=120, cost_usd=0.0012)\\n    mock_session = MockSession()\\n\\n    # Record events\\n    mock_session.record({\\n        \\\"event\\\": \\\"model_retry\\\",\\n        \\\"run_id\\\": mock_result.run_id,\\n        \\\"attempt\\\": 1,\\n        \\\"error\\\": \\\"API error\\\",\\n        \\\"timestamp\\\": benchmark_runner._utc_now_iso(),\\n    })\\n\\n    mock_session.record({\\n        \\\"event\\\": \\\"model_fallback\\\",\\n        \\\"run_id\\\": mock_result.run_id,\\n        \\\"primary_model\\\": \\\"gpt-4\\\",\\n        \\\"fallback_model\\\": \\\"gpt-3.5-turbo\\\",\\n        \\\"timestamp\\\": benchmark_runner._utc_now_iso(),\\n    })\\n\\n    # Persist to JSONL\\n    root = Path(__file__).resolve().parents[1]\\n    session_log_path = root / \\\".elephant-coder\\\" / \\\"sessions\\\" / \\\"test_session.jsonl\\\"\\n    session_log_path.parent.mkdir(parents=True, exist_ok=True)\\n\\n    with session_log_path.open(\\\"w\\\", encoding=\\\"utf-8\\\") as f:\\n        for event in mock_session.events:\\n            f.write(json.dumps(event) + \\\"\\\\n\\\")\\n\\n    # Verify persistence\\n    with session_log_path.open(\\\"r\\\", encoding=\\\"utf-8\\\") as f:\\n        lines = f.readlines()\\n\\n    assert len(lines) == 2\\n    for line in lines:\\n        event = json.loads(line)\\n        assert \\\"event\\\" in event\\n        assert \\\"run_id\\\" in event\\n\\n    # Cleanup\\n    session_log_path.unlink()\\n    session_log_path.parent.rmdir()\\n\"}], \"dependencies\": {\"dependencies\": {}, \"devDependencies\": {}, \"python\": [], \"pythonDev\": [], \"rust\": [], \"go\": [], \"cpp\": []}, \"commands\": [\"pytest tests/test_model_retry_fallback.py -v\"]}"}
