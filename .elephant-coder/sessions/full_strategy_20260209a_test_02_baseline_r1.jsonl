{"ts": "2026-02-09T00:25:25Z", "session_id": "full_strategy_20260209a_test_02_baseline_r1", "role": "user", "command": "/code", "run_id": "run_de8744e8e280", "content": "Write tests for model retry and fallback telemetry in /code responses."}
{"ts": "2026-02-09T00:25:25Z", "session_id": "full_strategy_20260209a_test_02_baseline_r1", "role": "assistant", "command": "/code", "run_id": "run_de8744e8e280", "content": "{\"summary\": \"Write tests for model retry and fallback telemetry in /code responses.\", \"project\": {\"language\": \"python\"}, \"files\": [{\"path\": \"tests/test_model_retry_fallback.py\", \"content\": \"\\\"\\\"\\\"Tests for model retry and fallback telemetry in /code responses.\\\"\\\"\\\"\\n\\nfrom __future__ import annotations\\n\\nimport importlib.util\\nimport sys\\nfrom pathlib import Path\\n\\n\\ndef _load_cli_module():\\n    root = Path(__file__).resolve().parents[1]\\n    cli_path = root / \\\"scripts\\\" / \\\"elephant_cli.py\\\"\\n    spec = importlib.util.spec_from_file_location(\\\"elephant_cli_local\\\", cli_path)\\n    if spec is None or spec.loader is None:\\n        raise RuntimeError(\\\"Unable to load scripts/elephant_cli.py\\\")\\n    module = importlib.util.module_from_spec(spec)\\n    sys.modules[spec.name] = module\\n    spec.loader.exec_module(module)\\n    return module\\n\\n\\nelephant_cli = _load_cli_module()\\n\\n\\ndef test_retry_and_fallback_telemetry_on_model_error(tmp_path: Path, monkeypatch):\\n    \\\"\\\"\\\"Verify telemetry records retry attempts and fallback model selection on transient errors.\\\"\\\"\\\"\\n    (tmp_path / \\\"pyproject.toml\\\").write_text(\\\"[project]\\\\nname='tmp'\\\\n\\\", encoding=\\\"utf-8\\\")\\n    monkeypatch.setenv(\\\"ELEPHANT_PROJECT_ROOT\\\", str(tmp_path))\\n    \\n    # Simulate transient model errors followed by success on fallback\\n    original_chat_completion = elephant_cli.OpenRouterClient.chat_completion\\n    call_count = 0\\n    \\n    def mock_chat_completion(self, *, model, messages, max_tokens, temperature=0.2, timeout_sec=60):\\n        nonlocal call_count\\n        call_count += 1\\n        if call_count <= 2:\\n            raise elephant_cli.OpenRouterError(\\\"Rate limit exceeded\\\")\\n        return original_chat_completion(self, model=model, messages=messages, max_tokens=max_tokens, temperature=temperature, timeout_sec=timeout_sec)\\n    \\n    monkeypatch.setattr(elephant_cli.OpenRouterClient, \\\"chat_completion\\\", mock_chat_completion)\\n    \\n    args = elephant_cli._parse([\\\"code\\\", \\\"--cwd\\\", str(tmp_path), \\\"--task\\\", \\\"add feature\\\", \\\"--output\\\", \\\"json\\\"])\\n    code, response = elephant_cli.execute_command(args)\\n    \\n    assert code == 0\\n    assert response[\\\"ok\\\"] is True\\n    telemetry = response.get(\\\"telemetry\\\", {})\\n    assert telemetry.get(\\\"model_retries\\\") == 2\\n    assert telemetry.get(\\\"model_fallbacks\\\") == 1\\n    assert telemetry.get(\\\"final_model\\\") in (\\\"gpt-4o-mini\\\", \\\"gpt-4o\\\")\\n\\n\\ndef test_fatal_error_does_not_retry(tmp_path: Path, monkeypatch):\\n    \\\"\\\"\\\"Verify no retry attempts for fatal model errors.\\\"\\\"\\\"\\n    (tmp_path / \\\"pyproject.toml\\\").write_text(\\\"[project]\\\\nname='tmp'\\\\n\\\", encoding=\\\"utf-8\\\")\\n    monkeypatch.setenv(\\\"ELEPHANT_PROJECT_ROOT\\\", str(tmp_path))\\n    \\n    original_chat_completion = elephant_cli.OpenRouterClient.chat_completion\\n    \\n    def mock_chat_completion(self, *, model, messages, max_tokens, temperature=0.2, timeout_sec=60):\\n        raise elephant_cli.OpenRouterError(\\\"Invalid API key\\\")\\n    \\n    monkeypatch.setattr(elephant_cli.OpenRouterClient, \\\"chat_completion\\\", mock_chat_completion)\\n    \\n    args = elephant_cli._parse([\\\"code\\\", \\\"--cwd\\\", str(tmp_path), \\\"--task\\\", \\\"add feature\\\", \\\"--output\\\", \\\"json\\\"])\\n    code, response = elephant_cli.execute_command(args)\\n    \\n    assert code != 0\\n    assert response[\\\"ok\\\"] is False\\n    telemetry = response.get(\\\"telemetry\\\", {})\\n    assert telemetry.get(\\\"model_retries\\\") == 0\\n    assert telemetry.get(\\\"model_fallbacks\\\") == 0\\n    assert telemetry.get(\\\"final_model\\\") == \\\"gpt-4o-mini\\\"\\n\\n\\ndef test_max_retries_limit_enforced(tmp_path: Path, monkeypatch):\\n    \\\"\\\"\\\"Verify retry stops after max_retries configured.\\\"\\\"\\\"\\n    (tmp_path / \\\"pyproject.toml\\\").write_text(\\\"[project]\\\\nname='tmp'\\\\n\\\", encoding=\\\"utf-8\\\")\\n    monkeypatch.setenv(\\\"ELEPHANT_PROJECT_ROOT\\\", str(tmp_path))\\n    \\n    original_chat_completion = elephant_cli.OpenRouterClient.chat_completion\\n    call_count = 0\\n    \\n    def mock_chat_completion(self, *, model, messages, max_tokens, temperature=0.2, timeout_sec=60):\\n        nonlocal call_count\\n        call_count += 1\\n        raise elephant_cli.OpenRouterError(\\\"Transient error\\\")\\n    \\n    monkeypatch.setattr(elephant_cli.OpenRouterClient, \\\"chat_completion\\\", mock_chat_completion)\\n    \\n    args = elephant_cli._parse([\\\"code\\\", \\\"--cwd\\\", str(tmp_path), \\\"--task\\\", \\\"add feature\\\", \\\"--output\\\", \\\"json\\\"])\\n    code, response = elephant_cli.execute_command(args)\\n    \\n    assert code != 0\\n    assert response[\\\"ok\\\"] is False\\n    telemetry = response.get(\\\"telemetry\\\", {})\\n    assert telemetry.get(\\\"model_retries\\\") == 2  # max_retries default\\n    assert telemetry.get(\\\"model_fallbacks\\\") == 1\\n    assert telemetry.get(\\\"final_model\\\") == \\\"gpt-4o-mini\\\"\\n\\n\\ndef test_fallback_model_selection_order(tmp_path: Path, monkeypatch):\\n    \\\"\\\"\\\"Verify fallback models are tried in configured order.\\\"\\\"\\\"\\n    (tmp_path / \\\"pyproject.toml\\\").write_text(\\\"[project]\\\\nname='tmp'\\\\n\\\", encoding=\\\"utf-8\\\")\\n    monkeypatch.setenv(\\\"ELEPHANT_PROJECT_ROOT\\\", str(tmp_path))\\n    \\n    original_chat_completion = elephant_cli.OpenRouterClient.chat_completion\\n    attempts: list[str] = []\\n    \\n    def mock_chat_completion(self, *, model, messages, max_tokens, temperature=0.2, timeout_sec=60):\\n        attempts.append(model)\\n        raise elephant_cli.OpenRouterError(\\\"Transient error\\\")\\n    \\n    monkeypatch.setattr(elephant_cli.OpenRouterClient, \\\"chat_completion\\\", mock_chat_completion)\\n    \\n    args = elephant_cli._parse([\\\"code\\\", \\\"--cwd\\\", str(tmp_path), \\\"--task\\\", \\\"add feature\\\", \\\"--output\\\", \\\"json\\\"])\\n    code, response = elephant_cli.execute_command(args)\\n    \\n    assert code != 0\\n    telemetry = response.get(\\\"telemetry\\\", {})\\n    assert telemetry.get(\\\"model_retries\\\") == 2\\n    assert telemetry.get(\\\"model_fallbacks\\\") == 1\\n    assert telemetry.get(\\\"final_model\\\") == \\\"gpt-4o-mini\\\"\\n    assert len(attempts) == 3  # default + 2 fallbacks\\n    assert attempts[0] == \\\"gpt-4o-mini\\\"\\n    assert attempts[1] == \\\"gpt-4o\\\"\\n    assert attempts[2] == \\\"gpt-4.1-nano\\\"\\n\\n\\ndef test_telemetry_includes_latency_and_tokens_per_attempt(tmp_path: Path, monkeypatch):\\n    \\\"\\\"\\\"Verify telemetry records latency and token usage per attempt.\\\"\\\"\\\"\\n    (tmp_path / \\\"pyproject.toml\\\").write_text(\\\"[project]\\\\nname='tmp'\\\\n\\\", encoding=\\\"utf-8\\\")\\n    monkeypatch.setenv(\\\"ELEPHANT_PROJECT_ROOT\\\", str(tmp_path))\\n    \\n    original_chat_completion = elephant_cli.OpenRouterClient.chat_completion\\n    attempt_data: list[dict[str, Any]] = []\\n    \\n    def mock_chat_completion(self, *, model, messages, max_tokens, temperature=0.2, timeout_sec=60):\\n        result = original_chat_completion(self, model=model, messages=messages, max_tokens=max_tokens, temperature=temperature, timeout_sec=timeout_sec)\\n        attempt_data.append({\\\"model\\\": model, \\\"latency_ms\\\": 100, \\\"tokens\\\": 50})\\n        return result\\n    \\n    monkeypatch.setattr(elephant_cli.OpenRouterClient, \\\"chat_completion\\\", mock_chat_completion)\\n    \\n    args = elephant_cli._parse([\\\"code\\\", \\\"--cwd\\\", str(tmp_path), \\\"--task\\\", \\\"add feature\\\", \\\"--output\\\", \\\"json\\\"])\\n    code, response = elephant_cli.execute_command(args)\\n    \\n    assert code == 0\\n    telemetry = response.get(\\\"telemetry\\\", {})\\n    assert \\\"attempts\\\" in telemetry\\n    assert len(telemetry[\\\"attempts\\\"]) >= 1\\n    for attempt in telemetry[\\\"attempts\\\"]:\\n        assert \\\"model\\\" in attempt\\n        assert \\\"latency_ms\\\" in attempt\\n        assert \\\"tokens\\\" in attempt\\n\"}"}
